{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ATL_G5LF53"
      },
      "source": [
        "## --------- Importing dependencies ---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmtyW6BDcKtD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tables\n",
        "import keras\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import DepthwiseConv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdASzNYRiGBD"
      },
      "source": [
        "## --------- Importing dataset ---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY33eoBb7xS9",
        "outputId": "d0ecf286-35b0-45c2-8b6a-2e19c91eb927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount the drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Connect to the drive containing the data\n",
        "INPUT_FOLDER = '/content/gdrive/Shareddrives/OurTeam/Source/data'\n",
        "os.listdir(INPUT_FOLDER)\n",
        "\n",
        "# Load the CSVs\n",
        "annotations = pd.read_csv(INPUT_FOLDER + '/metadata/annotations.csv')\n",
        "candidates = pd.read_csv(INPUT_FOLDER + '/metadata/candidates.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsAjswe8iJdv"
      },
      "source": [
        "## --------- Preprocessing data ---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwGXQdfm0JQe",
        "outputId": "fa90b006-6589-470e-fcc4-395facee17e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-dff6a664553d>:27: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  X_train_new = X_train.append(X_train.loc[positive_indexes])\n",
            "<ipython-input-3-dff6a664553d>:28: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  y_train_new = y_train.append(y_train.reindex(positive_indexes))\n"
          ]
        }
      ],
      "source": [
        "# Get positive and negative indexes\n",
        "positives = candidates[candidates['class'] == 1].index\n",
        "negatives = candidates[candidates['class'] == 0].index\n",
        "\n",
        "# Randomly select negative indexes to achieve 10:1 negative to positive ratio\n",
        "negIndexes = np.random.choice(negatives, len(positives) * 10, replace=False)\n",
        "\n",
        "# Combine positives and negative candidates\n",
        "candidatesDf = candidates.iloc[list(positives) + list(negIndexes)]\n",
        "\n",
        "# Split data into features (X) and target variable (y)\n",
        "X = candidatesDf.iloc[:, :-1]\n",
        "y = candidatesDf.iloc[:, -1]\n",
        "\n",
        "# Split data into training, testing, and validation sets\n",
        "rand_state = 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=rand_state)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=rand_state)\n",
        "\n",
        "# Save data as CSV files\n",
        "X_train.to_csv(INPUT_FOLDER + '/preprocessed_data/traindata.csv', index=False)\n",
        "X_test.to_csv(INPUT_FOLDER + '/preprocessed_data/testdata.csv', index=False)\n",
        "X_val.to_csv(INPUT_FOLDER + '/preprocessed_data/valdata.csv', index=False)\n",
        "\n",
        "# Augment data by duplicating positive instances\n",
        "positive_indexes = X_train[y_train == 1].index\n",
        "X_train_new = X_train.append(X_train.loc[positive_indexes])\n",
        "y_train_new = y_train.append(y_train.reindex(positive_indexes))\n",
        "\n",
        "# Generate train filenames for the dataset file\n",
        "train_filenames = X_train_new.index.map(lambda x: f\"{INPUT_FOLDER}/data/train/image_{x}.jpg\").astype(str)\n",
        "\n",
        "# Set the filename for the dataset file\n",
        "dataset_file = 'traindatalabels.txt'\n",
        "\n",
        "# Create a structured array to store filenames and labels\n",
        "traindata = np.zeros(train_filenames.size, dtype=[('filename', 'S36'), ('label', int)])\n",
        "traindata['filename'] = train_filenames.values.astype(str)\n",
        "traindata['label'] = y_train_new.values.astype(int)\n",
        "\n",
        "# Save the structured array as a text file\n",
        "np.savetxt(dataset_file, traindata, fmt=\"%10s %d\")\n",
        "\n",
        "# Read image and label data from HDF5 files\n",
        "with tables.open_file(INPUT_FOLDER + '/hdf5_data/train_dataset.h5', mode='r') as h5f:\n",
        "    X_train_images = h5f.root.X.read()\n",
        "    Y_train_labels = h5f.root.Y.read()\n",
        "\n",
        "with tables.open_file(INPUT_FOLDER + '/hdf5_data/val_dataset.h5', mode='r') as h5f2:\n",
        "    X_val_images = h5f2.root.X.read()\n",
        "    Y_val_labels = h5f2.root.Y.read()\n",
        "\n",
        "with tables.open_file(INPUT_FOLDER + '/hdf5_data/test_dataset.h5', mode='r') as h5f3:\n",
        "    X_test_images = h5f3.root.X.read()\n",
        "    Y_test_labels = h5f3.root.Y.read()\n",
        "\n",
        "# Convert image data to NumPy arrays\n",
        "X_train_images_np = np.expand_dims(np.array(X_train_images), axis=3)\n",
        "X_val_images_np = np.expand_dims(np.array(X_val_images), axis=3)\n",
        "X_test_images_np = np.expand_dims(np.array(X_test_images), axis=3)\n",
        "\n",
        "# Convert label data to NumPy arrays\n",
        "y_train_labels_np = np.array(Y_train_labels)\n",
        "y_val_labels_np = np.array(Y_val_labels)\n",
        "y_test_labels_np = np.array(Y_test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxAm7WHQiRZE"
      },
      "source": [
        "## --------- Defining the model architecture ---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt9NZ23OP66o"
      },
      "outputs": [],
      "source": [
        "def my_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(DepthwiseConv2D(64, (3, 3), padding='same', input_shape=(50, 50, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(DepthwiseConv2D(64, (3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(DepthwiseConv2D(64, (3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(DepthwiseConv2D(128, (3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(DepthwiseConv2D(128, (3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(DepthwiseConv2D(128, (3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(DepthwiseConv2D(256, (3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(DepthwiseConv2D(256, (3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(DepthwiseConv2D(256, (3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(512))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(256))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(2, activation='sigmoid')) \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = my_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYAL7oudkcHl"
      },
      "source": [
        "## --------- Training the model ---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R3RIc6RevQ-",
        "outputId": "9ffeb46f-4e58-43ab-d9a3-debe6ddfe819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ],
      "source": [
        "# Define optimizer\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(patience=10),\n",
        "    keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=5),\n",
        "    keras.callbacks.ModelCheckpoint(filepath='classification_model_best_weights.h5', save_best_only=True),\n",
        "]\n",
        "\n",
        "# Create data generator\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "\n",
        "# Prepare iterator for training data\n",
        "batch_size = 64\n",
        "it_train = datagen.flow(X_train_images_np, y_train_labels_np, batch_size=batch_size)\n",
        "\n",
        "# Calculate steps per epoch\n",
        "steps = X_train_images_np.shape[0] // batch_size\n",
        "\n",
        "# Fit the model to the training data\n",
        "hist = model.fit_generator(\n",
        "    it_train,\n",
        "    steps_per_epoch=steps,\n",
        "    epochs=5,\n",
        "    validation_data=(X_val_images_np, y_val_labels_np),\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2dbWSwEiev7"
      },
      "source": [
        "## --------- Testing the model ---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMHtkqt4w1Yu",
        "outputId": "c1bd78cd-cb8a-4a22-a7bb-db560be81d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 24s 556ms/step - loss: 0.6931 - accuracy: 0.8273\n"
          ]
        }
      ],
      "source": [
        "# Evaluate our model\n",
        "_, acc = model.evaluate(X_val_images_np, y_val_labels_np, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-shgj9iq01RK",
        "outputId": "a55fc84d-5076-4f2b-c577-acbf165bd336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Model Accuracy 82.729 ***\n"
          ]
        }
      ],
      "source": [
        "print('*** Model Accuracy %.3f ***' % (acc * 100.0))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}